#!/usr/bin/env python
# logappend
# rainy_day_hackers
# Sep 13, 2014
#

# assumptions:
# path to logappend

import argparse
import sys
import os.path
import call

parser = argparse.ArgumentParser(description='runs the provided executable and reports on expect results vs. actual')

parser.add_argument('-r', dest='logr', action='store', help="The path to enemy logread and filename")
parser.add_argument('-a', dest='loga', action='store', help="The path to enemy logappend and filename")
parser.add_argument('-g', dest='generic', action='store', help="generic path to enemy directory")


args, unknown = parser.parse_known_args()

# can we use our own test script?
if (len(unknown) or not (args.execu and args.batch)):
        print("rtfm!")
        print("The Manual: -r path/to/enemylogread -a path/to/enemylogappend")
        sys.exit(-1)

if not os.path.isfile(args.logr)
        print("path to enemy logread is wrong")
        sys.exit(-1)

if not os.path.isfile(args.loga)
        print("path to enemy logappend is wrong")
        sys.exit(-1)


# Overview
#
# a) we first invoke the -b version of the executable on willfail1
# b) then iterate (starting from beginning again) through willfail1 using exec without -b
# c) repeat steps a and b for willfail2
#
# More specifically
#
# However we need to create mini-batch versions so that we can determine simply where something fails. This
# means first parsing the willfail (which separates tests via //comments). We can use these comments as documentation
# for what kind of error we found. So, the lines between //comments will be created into minibatch files, the output
# of each can be more effectively evaluated because it is a limited case and always a unique guest/employee that is
# easily traced back if requried.


readfile = open(args.batch)
writefile = open(minibatch,"a")
count = 0
oldtestline=''
minibatch=args.generic+'/minibatch'

for testline in readfile:
    if testline[0] == "/":                              # divider
        count += 1                                      # which test are you on?
        call([args.logr, "-b", "minibatch", "1>", "tempresults"])    # run as batch put results somewhere
        if not(os.path.isfile(tempresults)):            # should generate output - every test fails.
            print("correctness violation at test {0} - {1}\n".format(count,oldtestline))
        else:
            if ("invalid" == open.tempresults.read() ): # invalid should be the only thing in there and only once
                print("correctness violation at test {0} - {1}\n".format(count, oldtestline))
        if count < 22:                                  # first 22 tests are one liners
            if os.path.isfile(log1):                    # was a log created? teams may handle differently
                if os.stat('log1').st_size > 0:
                    print("correctness violation at test {0} - {1}\n".format(count, oldtestline))
        else:                                           # generate expected output
            

        oldtestine = testine
        call(["rm", minibatch]
    else:                                               # make the new minibatch file
        open(minibatch,"a").write(testline)                 # append to mini batch file


